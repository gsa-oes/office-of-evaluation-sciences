---
title:  Evaluating Global Vaccine Demand and Uptake - A Collaboration between OES and CDC’s Demand for Immunization Team (DIT)
permalink: /collaborations/demand-for-immunization/
tags:
  - collaborations
image: /assets/img/cdc-gid.jpg
year: 2023
domain: Global Helth
agency: 
  - Health and Human Services
summary: 
featured: 
- homepage
---

## What was the challenge?

Most low- and middle-income countries (LMICs) are not prepared to meet 70% of the global COVID-19 vaccine coverage goals due to health systems challenges and vaccine hesitancy, and many have also experienced disruptions to routine immunization systems.¹ The Demand for Immunization Team (DIT) in the U.S. Centers for Disease Control and Prevention (CDC) Global Immunization Division (GID) seeks to evaluate global immunization demand interventions, specifically assessing the effectiveness of interventions using outcome and impact evaluations. This can be challenging for various reasons, including the fact that projects in the DIT portfolio occur in implementation environments where data quality is often highly variable. Additionally, there is often a need for rapid emergency response (e.g. increasing COVID-19 vaccine uptake) rather than a long-term engagement that addresses low vaccine uptake through collection of demand insights and rigorous evaluation. Doing so can help build the evidence base on vaccine demand and uptake, inform the funding strategy (e.g. for improved cost-effectiveness), and shape stakeholder commitment to specific interventions. 

## What did we do?

The Office of Evaluation Sciences partnered with the DIT to develop an internal practical guide that describes the various tasks a DIT team member could conduct and questions they could ask when undertaking an impact evaluation of vaccine demand interventions. In addition to outlining the pros, cons, and prerequisites for different causal evaluation and correlational (i.e. non-causal) designs, the practical guide (outlined below) also described steps to be undertaken and questions that users could ask of the program as they considered an evaluation: 

- *Before planning an intervention*
What is the current program and how does it target vaccine uptake? Are there approaches that can be varied or tailored to specific concerns and demographic populations?
How have you assessed factors contributing to low uptake of vaccines in the current program (e.g. using existing specific frameworks such as the Behavioral and Social Drivers (BeSD) of vaccination)?
In the case when an intervention is not designed, how is implementation and adaptation of existing evidence-based interventions supported for a new context, target population etc.? 
Once intervention is identified
How does the proposed intervention address drivers of vaccination? 
What is the proposed intervention expected to change (e.g. in terms of upstream demand/intent/uptake factors and downstream health benefits or cost savings) and can these changes be measured directly via existing data sources or do new data need to be collected?
Is there buy-in on evaluation design and rationale from relevant stakeholders?
Ingredients for an evaluation
Relevant questions on data access, collection, and data sharing
Are there differences between the current status quo and proposed intervention in terms of access, timing of access, encouragement, location etc.? This would address a potential opportunity to randomize and conduct an experimental evaluation
What are relevant individual level/behavior and system-level outcomes that can be monitored for this intervention? 
What are the pros/cons of different analytical approaches (including both causal and non-causal study designs)?

<img src="{{ '/assets/img/immunization-fig1.png' | prepend: site.baseurl }}" alt="" width="1500">
**Figure 1.** *Evaluation guide (screenshot from Table of Contents)*

## What did we learn?

The guide suggests key tips for thinking about conducting impact evaluations in this space, as outlined below:
Be transparent about the use of a causal or non-causal design (e.g. pre/post designs or multiple regression methods) and be aware of how it may (or may not) help assess intervention or program impact and the types of bias it introduces. It is important to do this while acknowledging that the implementation environment shapes the choice of study design and impact evaluation feasibility
Recognize which data sources can/cannot be used and for what purposes (e.g. the role of registry data for vaccine uptake may differ from that of survey measures for behavioral outcomes)
Situate the different roles of process indicators to measure what was done (i.e. to explain the fidelity of implementation) and qualitative research (i.e. to explain why an intervention did or did not work), but not whether it worked
There is potential to rigorously assess impact if there are differences (i.e. random variation) in how the current and proposed programs are set up in terms of access, timing, encouragement, location etc.
The guide may be used by the DIT in several ways: in scoping out new projects within GID and with its external partners, especially as the team builds, tests, and uses various tools to assess behavioral and social drivers of vaccine demand and uptake; in evaluating existing vaccine demand and uptake interventions with partners; and in shaping the research agenda with other programmatic and research stakeholders where DIT may request impact evaluations of interventions it supports. Ultimately, the DIT can also use estimates derived from impact evaluations alongside empirical cost data collected by other GID teams to estimate the cost-effectiveness and sustainability of vaccine demand interventions.
