---
title: Incorporating evaluation into digital forms
layout: page
permalink: /results/incorporating-evaluation-into-digital-forms/
tags: 
  - abstract
image: /assets/img/project-images/2206-image.webp
image-credit: https://unsplash.com/photos/7DziDCBnhiE
summary: 

---
<br>
<a class="usa-button" href="/assets/abstracts/2118-digital-forms-abstract.pdf" target="_blank">View the PDF Version</a>

## Target a priority outcome
The American public spends approximately 11.5 billion hours per year filling out federal forms.<sup>1</sup> Form complexity can result in lack of submission or completion, and errors on forms can cause processing delays and affect whether the form is accepted–which can have far-reaching consequences. Complex federal forms also place a large burden on government agencies who are responsible for processing responses, investigating errors, and verifying information. This evaluation takes an incremental step to build evidence and capacity on the testing of digital federal forms. While this study tests only one intervention, a central goal of this study was “proof-of-concept”
for building evidence to improve federal form design in the future.

## Translate behavioral insights
The burdens associated with completing a form can be reduced by providing clear instructions and utilizing effective formatting.<sup>2</sup> However, many respondents do not carefully read or follow instructions about how to complete a form, and when written instructions conflict with examples, respondents consistently use the example information and disregard instructions.<sup>3,4</sup> Embedding instructions alongside questions may make them more accessible (physically closer), more obviously relevant, and less demanding to process (shorter blocks of text). In contrast, embedding instructions alongside questions may also mean that people start answering questions without having read the full set of instructions. Providing instructions alongside questions may also entail using a link to pop-up those instructions, which may further decrease the likelihood that instructions are read, as they would require an additional effort to access. Rigorous evidence on how the positioning of instructions on a form affects responses is limited and relevant to most—if not all—federal forms. 

## Embed evaluation
We implemented a randomized control trial (RCT) to build evidence on the magnitude and direction of the effect of instruction positioning in federal forms. We evaluated two versions of a brief digital form which included questions typical of federal forms. One version included the form instructions on the first page, while the other version embedded the form instructions within each page of the form. 

To generate a sample of users, we conducted outreach among the general public and federal employees. Outreach included tweets, email newsletters sent to federal employees (i.e., GSAToday) and the general public (i.e., USAGov), listservs, a pop-up on forms.gov, and a posting on challenge.gov. Individuals voluntarily chose to participate by clicking a link to the form that was included in the tweet, on the site, or in the email.<sup>5</sup> Between July 19 and August 19, 2022, there were 3,203 instances in which an individual clicked on a link to fill out the form. An online platform randomly assigned individuals who clicked on the form link to either the form with embedded instructions or instructions at the front.<sup>6</sup>

## Analyze using existing data
The primary outcome of the evaluation is form submission, defined as starting and submitting the form.<sup>7</sup> The online platform was used to identify the total number of individuals who were randomized to each of the two forms and form response data was used to measure form submission.<sup>8</sup>

## Results
The results offer initial evidence that form completion is affected by instruction placement.<sup>9</sup> Across both types of form, almost two-thirds of users who started the form did not submit it. Individuals randomly assigned to the form with embedded instructions were 3.2 percentage points (36.2% versus 33.0%) more likely to submit (p=0.054, 95% CI [-0.001, 0.065]) than individual assigned to the form with instructions at the front, with this result significant at the p<0.10 level. 

The pilot study also showed that conducting an A/B test of a typical federal form using a sample of voluntary users recruited from the general public is possible. Over 3,000 users started the form and 1,110 users submitted the form in a one month period. More control over the assignment mechanism and access to more complete outcome data would be beneficial in future studies. Over the examined study period, 93 more users (3% of the total sample) were assigned to the embedded instructions version of the form than were assigned to the instructions at the beginning version of the form. One possible contributor to this imbalance was internal testing of the form design and mechanics. However, this cannot be confirmed without additional outcome data and direct observation of the assignment process.

<b>Figure 1:</b> Form submission by form version

## Build evidence
This evaluation — a first of its kind in the federal government—brought together multiple GSA offices and the American public to learn about the feasibility of incorporating A/B testing into federal forms and to show that form design matters for form completion. The empirical findings indicate that where instructions are placed impacts form submission, a substantive finding on the most fundamental outcome on filling out a form. Agencies could help substantiate this finding by continuing to evaluate the effects of instruction placement on high priority forms.

This pilot on the feasibility of incorporating A/B testing uncovered opportunities (e.g., public enthusiasm for improving federal forms) and challenges (e.g., limited control over the randomization process and access desired outcome data, such as time to completion and capturing incomplete responses). This evaluation showed federal forms are ripe for improvement and evidence-building activities that inform form design could reduce burdens on the public. While interest in improving federal forms among the federal government and public is high, more work is needed to improve the testing infrastructure of federal forms in order to build and apply rigorous evidence to improve form design.

*Notes:*
1. I.e., individuals receiving Social Security Disability Insurance and/or Supplemental Security Income benefits based on disability.
2. U.S. Government Accountability Office. "Ticket to Work Helped Some Participants, but Overpayments Increased Program Costs." <a href="https://www.gao.gov/assets/gao-22-104031.pdf">Report to Congressional Committees (2021).</a>
3. <a href="https://www.mathematica.org/publications/characteristics-employment-and-sources-of-support-among-workingage-ssi-and-di-beneficiaries">Mathematica. Characteristics, Employment, and Sources of Support Among Working-Age SSI and DI Beneficiaries. April 30th, 2009.</a>
4. Office of Evaluation Sciences. How to design effective communications: What has OES learned? Bhargava, S., & Manoli, D. (2015). Psychological frictions and the incomplete take-up of social benefits: Evidence from an IRS field experiment. _American Economic Review_, 105(11), 3489-3529.
5. These cardstock Tickets were included in the original TTW mailings, but were eliminated due to budget constraints. The alternative (status quo) is a paper Ticket including more text.
6. These sample sizes incorporate exclusions due to unanticipated missing data and logistical issues. Analyses that handle those issues differently yield similar findings.
7. All statistically significant results we report remain so after applying our <a href="https://oes.gsa.gov/assets/analysis/1902-analysis-plan.pdf">pre-registered</a> multiple testing correction.

